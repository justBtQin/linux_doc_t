CFQ（完全公平队列）

===============================

CFQ 调度器的主要目标是为所有请求 I/O 操作的进程提供磁盘 I/O 带宽的公平分配。CFQ 为请求 I/O（同步请求）的进程维护每个进程的队列。对于异步请求，根据进程的 I/O 优先级将所有进程的请求批量在一起。

# CFQ ioscheduler 可调参数

========================

slice_idle

----------

这指定了在某个 cfq 队列（对于顺序工作负载）和服务树（对于随机工作负载）上，CFQ 在过期并选择下一个要调度的队列之前应该空闲多长时间。

默认情况下，slice_idle 是非零值。这意味着默认情况下我们在队列/服务树上空闲。这在像单个磁盘的 SATA/SAS 磁盘这样的高寻道媒体上非常有帮助，因为我们可以减少总的寻道次数并提高吞吐量。

将 slice_idle 设置为 0 将在队列/服务树级别消除所有空闲，并且在硬件 RAID 配置中的多个 SATA/SAS 磁盘等更快的存储设备上应该会看到总体吞吐量的提高。缺点是从写入提供的隔离也会降低，I/O 优先级的概念也会变弱。

因此，根据存储和工作负载的不同，将 slice_idle 设置为 0 可能会很有用。一般来说，我认为对于 SATA/SAS 磁盘和 SATA/SAS 磁盘的软件 RAID，保持 slice_idle 启用应该是有用的。对于在单个 LUN 后面有多个磁盘（基于主机的硬件 RAID 控制器或存储阵列）的任何配置，将 slice_idle 设置为 0 可能会最终获得更好的吞吐量和可接受的延迟。

back_seek_max

-------------

这指定了以千字节为单位的向后寻道的最大“距离”。距离是从当前磁头位置到在距离上向后的扇区的空间量。

此参数允许调度程序预测“向后”方向的请求，并在它们在当前磁头位置的此距离内时将它们视为“下一个”请求。

back_seek_penalty

-----------------

此参数用于计算向后寻道的成本。如果请求的向后距离仅为“前面”请求的 1/back_seek_penalty，那么这两个请求的寻道成本被认为是等效的。

因此，调度程序不会偏向于一个或另一个请求（否则调度程序将偏向于前面的请求）。back_seek_penalty 的默认值为 2。

fifo_expire_async

-----------------

此参数用于设置异步请求的超时时间。此参数的默认值为 248ms。

fifo_expire_sync

----------------

此参数用于设置同步请求的超时时间。此参数的默认值为 124ms。如果要偏爱同步请求而不是异步请求，则应相对于 fifo_expire_async 减小此值。

group_idle

-----------

此参数强制在 CFQ 组级别而不是 CFQ 队列级别空闲。这是在高端存储中由于顺序队列上的空闲而观察到瓶颈后引入的，允许从单个队列进行调度。此参数的想法是它可以与 slice_idle=0 和 group_idle=8 一起运行，以便在组中的各个队列上不发生空闲，但在整个组上发生空闲，从而仍然使 I/O 控制器工作。不在组中的各个队列上空闲将同时从组中的多个队列调度请求，并在高端存储上实现更高的吞吐量。

此参数的默认值为 8ms。

latency

-------

此参数用于启用/禁用 CFQ 调度器的延迟模式。如果启用延迟模式（称为低延迟），CFQ 将尝试根据为系统设置的 target_latency 为每个进程重新计算切片时间。这有利于公平性而不是吞吐量。禁用低延迟（将其设置为 0）将忽略目标延迟，允许系统中的每个进程获得完整的时间片。

默认情况下，低延迟模式处于启用状态。

target_latency

--------------

此参数用于在 cfq 的延迟模式启用时计算进程的时间片。它将确保同步请求具有估计的延迟。但是，如果顺序工作负载较高（例如顺序读取），则为了满足延迟约束，由于每个进程在 cfq 队列切换之前发出 I/O 请求的时间较少，吞吐量可能会降低。

虽然可以通过禁用延迟模式来克服此问题，但这可能会增加某些应用程序的读取延迟。此参数允许通过 sysfs 接口更改 target_latency，以提供平衡的吞吐量和读取延迟。

target_latency 的默认值为 300ms。

slice_async

-----------

此参数与 slice_sync 相同，但适用于异步队列。默认值为 40ms。

slice_async_rq

--------------

此参数用于限制在队列的切片时间内将异步请求调度到设备请求队列的数量。允许调度的最大请求数也取决于 I/O 优先级。此参数的默认值为 2。

slice_sync

----------

当选择一个队列进行执行时，在切换到另一个队列之前，队列的 I/O 请求仅在一定时间（time_slice）内执行。此参数用于计算同步队列的时间片。

time_slice 使用以下公式计算：time_slice = slice_sync + (slice_sync/5 * (4 - prio))。要增加同步队列的 time_slice，请增加 slice_sync 的值。默认值为 100ms。

quantum

-------

这指定了调度到设备队列的请求数量。在队列的时间片内，如果设备中的请求数量超过此参数，则不会调度请求。此参数用于同步请求。

对于具有多个磁盘的存储，此设置可以限制请求的并行处理。因此，增加此值可以提高性能，但由于请求数量增加，可能会导致某些 I/O 的延迟增加。

# CFQ 组调度

====================

CFQ 支持 blkio cgroup，并且在每个 blkio cgroup 目录中都有“blkio.”前缀的文件。它是基于权重的，并且有四个旋钮用于配置 - weight[_device] 和 leaf_weight[_device]。内部 cgroup 节点（有子节点的节点）也可以在其中有任务，因此前两个配置 cgroup 作为整体在其父级级别有权获得的比例，而后两个配置 cgroup 中的任务与直接子节点相比的比例。

另一种思考方式是假设每个内部节点都有一个隐式的叶子子节点，该节点承载所有权重由 leaf_weight[_device] 配置的任务。假设一个由五个 cgroup 组成的 blkio 层次结构 - root、A、B、AA 和 AB - 具有以下权重，其中名称表示层次结构。

        weight leaf_weight

 root :  125    125

 A    :  500    750

 B    :  250    500

 AA   :  500    500

 AB   : 1000    500

root 永远没有父级，因此其权重是无意义的。为了向后兼容，weight 始终与 leaf_weight 保持同步。B、AA 和 AB 没有子节点，因此其任务没有子 cgroup 可竞争。它们始终获得 cgroup 在父级级别赢得的 100%。仅考虑重要的权重，层次结构如下所示。

          root

       /    |   \

      A     B    leaf

     500   250   125

   /  |  \

  AA  AB  leaf

 500 1000 750

如果所有 cgroup 都有活动的 I/O 并相互竞争，磁盘时间将如下分布。

根以下的分布。此级别上的总活动权重为 A:500 + B:250 + C:125 = 875。根叶：125 / 875  ≈ 14%

A：500 / 875  ≈ 57%

B（-叶）：250 / 875  ≈ 28%

A 有子节点，并将其 57%进一步分配给子节点和隐式叶节点。此级别的总活动权重为

AA：500 + AB：1000 + A - 叶：750 = 2250。

A - 叶：(750 / 2250) * A ≈ 19%

AA（-叶）：(500 / 2250) * A ≈ 12%

AB（-叶）：(1000 / 2250) * A ≈ 25%

# 用于组调度的 CFQ IOPS 模式

===================================

基本的 CFQ 设计是提供基于优先级的时间片。较高优先级的进程获得较大的时间片，较低优先级的进程获得较小的时间片。如果存储速度快并支持 NCQ，则测量时间会变得更加困难，并且最好一次从请求队列中的多个 cfq 队列调度多个请求。在这种情况下，无法准确测量单个队列消耗的时间。

然而，可能的是测量从单个队列调度的请求数量，并且同时允许从多个 cfq 队列调度。这实际上在 IOPS（每秒 IO 操作）方面变得公平。

如果将 slice_idle = 0 并且存储支持 NCQ，则 CFQ 内部切换到 IOPS 模式并开始在调度的请求数量方面提供公平性。请注意，此模式切换仅对组调度有效。对于非 cgroup 用户，不应有任何更改。

# CFQ IO 调度程序空闲理论

===============================

在队列上空闲主要是在完成请求后等待下一个请求出现在同一队列上。在此过程中，即使其他 cfq 队列中有挂起的请求，CFQ 也不会从其他 cfq 队列调度请求。

空闲的基本原理是它可以减少旋转介质上的寻道次数。例如，如果一个进程正在进行依赖的顺序读取（下一次读取仅在之前的读取完成后才会发生），那么不从其他队列调度请求应该会有所帮助，因为我们没有移动磁盘头并且一直在从一个队列调度顺序 IO。

CFQ 具有以下服务树，并且各种队列被放置在这些树上。

    sync-idle    sync-noidle    async

所有进行同步顺序 IO 的 cfq 队列都转到 sync-idle 树。在这棵树上，我们对每个队列单独进行空闲。

所有同步非顺序队列转到 sync-noidle 树。此外，任何带有 REQ_NOIDLE 标记的请求都转到此服务树。在这棵树上，我们不在单个队列上进行空闲，而是在整个队列组或树中进行空闲。因此，如果有 4 个队列等待 IO 进行调度，我们将仅在最后一个队列调度 IO 并且此服务树中没有更多 IO 时进行一次空闲。

所有异步写入转到 async 服务树。异步队列上没有空闲。

CFQ 对 SSD 有一些优化，如果它检测到可以支持更高队列深度（一次有多个请求处于活动状态）的非旋转介质，则它会减少单个队列的空闲，并且所有队列都移动到 sync-noidle 树，并且仅保留树空闲。此树空闲提供了与 async 树中的缓冲写入队列的隔离。

# 常见问题解答

===

Q1. 为什么要在带有 REQ_NOIDLE 标记的队列上进行空闲？

A1. 我们仅在带有 REQ_NOIDLE 标记的队列上进行树空闲（sync-noidle 树上的所有队列）。这有助于与所有 sync-idle 队列提供隔离。否则，在存在许多顺序读取器的情况下，其他同步 IO 可能无法获得磁盘的公平份额。

    例如，如果有 10 个顺序读取器进行 IO 并且它们每个获得 100ms。如果一个 REQ_NOIDLE 请求进入，它将大约在 1 秒后被调度。如果在 REQ_NOIDLE 请求完成后我们不进行空闲，并且在几毫秒后另一个 REQ_NOIDLE 请求进入，它将再次在 1 秒后被调度。重复此操作，并注意工作负载如何失去其磁盘份额并因多个顺序读取器而遭受损失。

    fsync 可以生成依赖的 IO，其中在 fsync 的上下文中写入一堆数据，然后稍后写入一些日志记录数据。日志记录数据仅在 fsync 完成其 IO 后才会进入（至少对于 ext4 似乎是这样）。现在，如果由于 REQ_NOIDLE 而决定不在 fsync 线程上进行空闲，那么下一个日志记录写入将在另一秒后才会被调度。进行小的 fsync 的进程在存在多个顺序读取器的情况下将遭受严重影响。

    因此，在使用请求上的 REQ_NOIDLE 标志的线程上进行树空闲提供了与多个顺序读取器的隔离，同时我们不在单个线程上进行空闲。

Q2. 何时指定 REQ_NOIDLE

A2. 我认为只要在进行同步写入并且不期望很快从同一上下文调度更多写入时，就应该能够在写入上指定 REQ_NOIDLE，并且这在大多数情况下可能效果很好。